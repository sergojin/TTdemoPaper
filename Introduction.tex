
\section{Introduction \label{sec:intro}}


[The concept of triggers]
Over their lifetime most high-energy physics experiments, such as those operating at hadron colliders, are exposed to ever increasing event rates. Due to the large size, the complexity, and the high granularity of their various detector components, the output data rate from these experiments can be staggering. As a result, processing and recording the vast amount of data for every event seen by these detectors is practically impossible. Collider experiments have dealt with this high data flux by implementing real-time event filters, or triggers, that reduce the flux to manageable levels for offline processing. 

These triggers are critical to the success of the physics program of the experiments as they allow selecting only those events with which important measurements or discoveries can be made. To that end, the reconstruction of all physics objects in each event has to be carried out as accurately and as fast as possible. To accomplish that, collider experiment triggers are typically designed so that decisions are made in sequential steps, or levels. Through this scheme, each successive level confronts a significantly reduced event rate with respect to the previous one and has access to more detailed and more complete information.

[LHC and CMS]
During Run 2 of the LHC, stretching from 2015 through 2018, the instantaneous luminosity provided by the accelerator at the CMS and ATLAS experiments has reached values of up to 2x1034 cm-2s-1, corresponding to a proton-proton collision rate of approximately 2 GHz; resulting from the 40 MHz accelerator bunch-crossing rate, with about 50 proton-proton interactions per bunch crossing on average.  At the CMS experiment each event [bunch crossing or event size?] generates about [1Mb] of [raw] data in the detector, from XXXX electronic channels [point out different numbers for trigger and for DAQ -- both for number of channels and for event size]. The total data rate coming from the detector is XXXX Tb/sec for the DAQ and YYYY Tb/sec into the trigger system. The rate is dominated by the tracking detectors, which are comprised of ZZZZ electronic channels. 

The CMS trigger is a two-level decision system, the level 1 (L1) and the High-Level Trigger (HLT), each rejecting the less interesting events, and each providing a rate reduction of about a factor of 100. The 40 MHz event rate is thus reduced to about 1 KHz.

[LHC and CMS Phase-2]

In order to vastly expand the physics program of the ATLAS and CMS experiments, the LHC and the experimental collaborations are pursuing a very significant upgrade program called Phase-2, with an expected completion date of 2025. This will usher the so-called HL-LHC era (for High-Luminosity LHC) during which the luminosity will increase by a factor of five or so relative to the Run 2 luminosity, resulting in an average number of proton-proton collisions per bunch crossing higher than 200. To contend with such a serious increase in collision rates and detector occupancies several of the detector systems of the ATLAS and CMS experiments are being upgraded. In particular the trigger systems need to undergo a significant redesign so that, in spite of the added event complexity due to such high pileup, the physics program remains viable and can possibly even be improved. 

[The Track Trigger concept]

As part of its Phase-2 set of upgrades, the CMS experiment is planning on including the new outer tracker detector envisioned for the HL-LHC era in the L1 trigger. The ability to reconstruct tracks at the L1 trigger, rather than at the HLT as currently implements, will do this for the experiment [see TDR]:
•	A –physics objects
•	B – lower pt
•	C – met/ particle flow 
•	Etc.

However, this is extremely challenging due to

•	A – number of tracks, rate etc
•	B – latency needed
•	C – efficiency/precision needed
•	Etc.

[Double sided concept/stubs]

To overcome these challenges the outer tracker modules have been designed such that the produce stubs rather than hits [expand on this]… 
The rate reduction from hits to stubs is about a factor of 10. 

[tracker geometry figure and module/stub concept figure]

[Divide in space and time]

Still, the stub rates are… the trigger architecture is therefore still in need of further parallelization, this is achieved by processing different regions of the detector in parallel and also by processing in parallel events from different bunch crossings [spaced by 25 ns]. All stubs to be processed together in a particular space/time region need to be brought to the same electronics platform at the same time

[AM solution]

In what follows we describe an end-to-end platform to solve the tracking problem at the L1 trigger for the CMS experiment. This solution is based on Associative Memories to perform a pattern recognition algorithm that filters out stubs that cannot be associated to real tracks. As part of the platform we present several innovative hardware and algorithmic developments that… The full chain of developments has been tested successfully through a vertical slice demonstrator that is described in more detail in what follows.



***** Below is the old text, to be merged *****

\subsection{Motivation}


In order to maximize the potential for the discovery, CMS must preserve or improve its ability to identify, in real time, events with signatures consistent with the Higgs boson and new particle decays for high luminosity LHC (HL-LHC) operation. This is a highly non-trivial task given the high pile-up conditions anticipated in the HL-LHC era. At LHC, the only major detector not used at L1 trigger are the silicon tracking detectors. It has become clear that the development of the L1 tracking trigger system is of utmost importance for CMS for the HL-LHC, in order to maintain physics acceptances for basic objects (leptons, photons, jets and MET).  Without L1 tracking, most quantities traditionally used for L1 are washed out and become unusable due to the huge "background" created by the overlap of too many collisions in the same beam crossing. This would put most of the anticipated physics program out of reach. 

Consequently, the design of the Phase-II CMS Tracker must allow for an effective implementation of the tracking trigger.... (add a few sentences here).  Because the construction of the Phase-II Tracker will take many years, its tracker design must be finalized sooner.    Silicon-based Level-2 tracking trigger systems based on associative memory were successfully implemented in the past~\cite{bib:Rist-89}~\cite{bib:Ade-06}~\cite{bib:Ade-07} and are being actively explored at present~\cite{bib:FTK-TP}~\cite{bib:FTK-TDR}. Experience with these systems will serve as useful input to the design of the CMS L1 tracking trigger. However the higher occupancies anticipated at the HL-LHC and the low latencies required at L1 (about 10~$\mu$s total and a few $\mu$s for the track finding stage) present us with a formidable set of challenges that we need to attack with a well organized R\&D campaign in order to be successful. A silicon-based L1 tracking trigger has never been realized at this scale and thus it is imperative that its feasibility be demonstrated before the design of the Phase-II Tracker can be finalized. To achieve this goal, a Vertical Slice System Demonstration system has been designed and built, using the assocative memory approach, and the  system comprises a full tracking trigger path and uses simulated high-luminosity data to measure trigger latency and efficiency, to study overall system performance and to identify appropriate solutions to possible bottlenecks. The demonstration system is by no means meant to be final  but serves the purpose of an existence proof.

\subsection{Associative Memory approach}

The Associative Memory solves the combinatorial problem, inherent to this kind of pattern recognition algorithms, by employing a massively parallel architecture to compare each detector hit to a large number of pre-calculated geometrical patterns simultaneously. Then, the selected patterns or roads are processed using fast FPGAs to perform track fitting. Since each pattern corresponds to a very narrow "road" through the detector, the usual helical fit is much simplified and fast by using a pre-calculated set of parameter values for the center of the road and applying corrections that are a linear function of the actual hit positions in each layer.  Because roads are narrow, this linear approximation works very well and the track fitting stage is much simplified and fast~\cite{bib:Ann-09} using pre-calculated track parameters for hits in the center of the road, and applying corrections that are linear in the exact position of the hits in each layer. Although roads are narrow, there is still a finite probability that multiple hits may fall within the same road for a given detector layer, requiring multiple fits with different hit combinations and leading to longer execution times. To reduce latency, the probability of occurrence of multiple hits in the same road must be made as small as possible by making the roads as narrow as possible and, consequently, increasing the total number of them we need to store in the AM to cover the parameter space of interest. This is why an aggressive R\&D program focused on achieving higher AM densities is an important component of the effort needed to reach the unprecedented low latencies required for silicon based tracking at the HL-LHC crossing frequency.

As soon as all the hits have been stored in the AM, found tracks are ready to be output. The whole latency incurred is the time needed to load the hits plus the time to read the matched patterns. In this sense the AM is hard to beat for this particular task. However, the design of an Associative Memory system capable of dealing with the much higher complexity of the HL-LHC collisions, and with the much shorter latency required by Level 1 triggering, poses significant, still unsolved, technical challenges. While we have a very aggressive R\&D program at Fermilab to advance the state-of-the-art associative memory technology (the 3D VIPRAM~\cite{bib:VIP-11} R\&D), we are open to possible new alternative approaches. Since the Associative Memory approach is a proven solution to tracking triggers in a hadron collider environment, it is chosen as the baseline for this demonsttration. 



\noindent 



%Main components

%Fiber data transmission,
%ATCA crates,
%Full mesh backplane,
%Fiber receivers,
%Pattern Recognition Boards,
%Pattern Recognition Mezzanines,
%AM as a possible incarnation (3d technology),
%Inter-tower communication,

%Bandwidth and latency considerations


\clearpage
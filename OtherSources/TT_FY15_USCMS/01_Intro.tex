\section{Technical Description and Deliverables}

\subsection{Introduction}


\noindent 

For CMS at HL-LHC, the bandwidth required to bring all data from the massive outer silicon detector reaches 100 Tbps.  Every 25 ns, all tracks with $p_{T}$ above $2\rm~GeV/c$ from (on average) 140 interactions need to be fully reconstructed.
Current estimates show that only a few microseconds will be available for L1 trigger processing.  This includes the time needed for data dispatch to the trigger towers, pattern recognition, track fitting.  Because each of these processes must be accomplished within a very short time, communication between processing elements in different towers requires very high bandwidth and very low latency. Extremely fast and effective track fitting is also required.  This is as high performance as computing gets.  L1 tracking for the HL-LHC will require the most advanced real time processing technology.



%As mentioned earlier, the CDF SVT-style Associative Memory chip (we will call it PRAM, Pattern Recognition Associative %Memory, to emphasis its purpose for HEP) is a departure beyond conventional CAMs. Like conventional CAMs, PRAMs store %address patterns and look for matches between incoming hits and those addresses for a given detector layer. At this level, the %match is expected to be either exact (Binary CAM) or partial (Ternary CAM) and an array of Match Flags is the typical output. A %PRAM has an array of Match Flag Latches which capture and hold the results of the match until reset for the next event. As the %hits from the various layers of the detector for the same event arrive, the PRAM is looking for matches from one candidate %address to one or more stored address patterns. The PRAM organizes stored address patterns into roads, which are linked %arrays of several stored address patterns from different detector layers. Each stored address in a road is from a different layer %in the detector system. These linked addresses represent a path that a particle might traverse through the layers of the %detector (hence the name "road"). The ultimate goal of the PRAM is to match real particle trajectories to those roads. Like a %conventional CAM, a PRAM flags a match when a candidate address matches a stored pattern address for a given detector %layer. However, before the PRAM does anything with that match, it must find matches in all (or majority of) the elements %(layers) that constitute a road. It should be emphasized that compared to commercially available CAMs, such as Network %Search Engine, the PRAM has the unique ability to search for correlations among input words received on different clock cycles. 
%This is essential for tracking trigger applications since the input words are the detector hits arriving from different layers at %different times. They arrive at the chip without any specific timing correlation. Each pattern has to store each fired layer until %the pattern is matched or the event is fully processed. Even in the case of a level-1 trigger application, which is largely %synchronous, this feature will still be important. One unique feature of this approach is that the pattern recognition of the %event is done as soon as the last hit arrives, which makes the approach a promising candidate for L1 track trigger. However, %the requirements for L1 track trigger application will be very different from that for L2, and the system interface of the chip has %to be fully redesigned and the performance has to be optimized.

In the traditional SVT-like AM approach, pattern recognition is solved by the Associative Memory, while track fitting is done in FPGAs using a linear approximation of the dependence of the track parameters on the exact location of the hits within each road. Because roads are narrow, this linear approximation works very well and the track fitting stage is much simplified and fast~\cite{bib:Ann-09} using pre-calculated track parameters for hits in the center of the road, and applying corrections that are linear in the exact position of the hits in each layer. Although roads are narrow, there is still a finite probability (especially with high occupancy at HL-LHC) that multiple hits may fall within the same road for a given detector layer, requiring multiple fits with different hit combinations and leading to longer execution times. To reduce latency, the occurrence of multiple hits from the same detector layer in the same road must minimized.  Consequently, roads must be made as narrow as possible, which requires higher number of patterns that must be stored in the AM. This is why an aggressive R\&D program focused on achieving higher AM densities is an important component of the effort needed to reach the unprecedented low latencies required for silicon based tracking at the HL-LHC.

The PRAM pattern density can be improved by optimizing the design in single-layer chips (2D), using custom cell designs with smaller feature size technology. An R\&D effort on-going at INFN aims to use 65 nm technology to improve AM design for the ATLAS FTK application (a L2 tracking trigger).  INFN now has in hand a 65 nm AM prototype, the AMchip05.  A predecessor, the INFN AM06, is expected to be submitted in Spring 2015. The AMchip05 or AM06 have not been designed or optimized for L1 track trigger applications; however, if availble, these chips could be used for the initial testing of the CMS L1 tracking trigger demonstration system.

\noindent The on-going R\&D effort at Fermilab explores the use of both conventional 2D and emerging 3D technology in the design of a future generation of PRAM chip (based on the VIPRAM~\cite{bib:VIP-11},~\cite{bib:VIP-12} approach) specifically for the needs of the L1 CMS tracking trigger. The Fermilab VIPRAM R\&D project has two goals.  The first is to increase in pattern density through the use of vertical integration and circuit and geometrical (layout) enhancements. This project will continue through FY15 for proof-of-principle of the 3D VIPRAM concept and is funded by DOE CDRD. The second is to increase speed and to improve the system interface, specifically with regard to Level 1 Tracking Trigger applications for CMS at HL-LHC.


%\noindent The architecture we propose is based on ATCA with a full-mesh backplane. The large inter-board communication %bandwidth provided by the full-mesh backplane is used to time multiplex the high volume of incoming data in such a way that %the I/O bandwidth demands are manageable at the board and chip level, making it possible for an early technical %demonstration with existing technology. The resulting architecture is scalable, flexible and open. For example, it allows %different pattern recognition architectures and algorithms to be explored and compared within the same platform. 

%For these reasons, although the system we propose is designed to allow the experimentation of different solutions to fast track %finding and fitting, we consider the Associative Memory approach as the baseline against which other new methods will be %compared. However, the design of an Associative Memory system capable of dealing with the much higher complexity of the %HL-LHC collisions, with an input event rate several orders of magnitude larger than ever done before, and with the much %shorter latency required by Level 1 triggering, poses significant, still unsolved, technical challenges. In fact, the entire system %design as well as board and chip level design will be very different for L1 tracking trigger comparing with that of L2 tracking %trigger applications.  An aggressive R\&D program is needed and is currently ongoing at Fermilab, based on quite a few years %of generic R\&D effort already done.

\noindent Establishing international collaborations within CMS to work on this project is essential. 
For most recent status of tracking trigger activities within CMS, please see presentations at the recent joint meeting of front-end, TK-DAQ and Track Finding meeting~\cite{bib:joint-meeting}, a summary of L1 track trigger progress report is also available~\cite{bib:summary}. Both INFN/Italy and Lyon/France have joined us to work on the vertical slice demonstration using associative memory approach, while others are interested in contributing or exploring possible new track finding algorithms on the same hardware platform. For example, INFN Italy has been working together with the Atlas FTK team exploring the possibility of using FTK associative memory chips for the CMS L1 tracking trigger demonstration. INFN is working closely with Fermilab to develop the pattern recognition mezzanine for the Pulsar II board to host the FTK AMchips (note that another mezzanine is being designed specifically for the associative memory chips being developed at Fermilab).  Experience gained with the FTK AMchips will be useful for guiding the design of a dedicated, CMS L1 associative memory chip at Fermilab. 

Beisdes the hardware development, one of the main activities in the coming year in USCMS (FY2015) will consist of extensive simulation efforts done by physicists, in order to to establish technical specifications based on Phase 2 physics goals. Due to the intrinsic massive parallel processing hardware nature of the AM operation, there is a clear challenge in using software based simulation to emulate the hardware performance. The development of simulation tools for the Associative Memory approach in CMS has been so far lead by the Lyon group, with significant progress made recently towards making the machinery work. The AM Simulation Camp~\cite{bib:AM-CAMP} at CERN in July 2014 has attracted many people and groups to attend, and many from USCMS. However, much of the work still remain to be done. Fermilab/LPC/USCMS have been working and will be more involved in the simulation efforts in FY2015.






\section{L1 Tracking Trigger}

comment/note: 
this is a rough draft really meant as a target for people to shoot at. not easy to write something meaningful in 2 pages.   I didnt include
things that i think is important. but let's get the discussion going among ourselves first. please forgive me if what I have here is not good at all...:)


\noindent 


	The design of the Phase-II CMS Tracker must allow effective implementation of the tracking trigger. The construction of the Phase-II Tracker is expected to take many years, therefore its design must be finalized soon. Silicon-based Level-1 tracking trigger has never been realized at such a large scale and thus it is imperative that its feasibility is demonstrated before the design of the Phase-II Tracker is finalized.  Silicon-based Level-2 tracking trigger systems based on associative memory were successfully implemented in the past and are being actively explored at present. Experience with these systems will provide useful input to the design of the CMS L1 tracking trigger. However, high detector occupancies anticipated in the HL-LHC era and the low latencies required at L1 (about 10~$\mu$s total and a few $\mu$s for the track finding stage) present a formidable set of challenges that need to be attacked with a well organized R\&D campaign in order to be successful. 

	Processing each beam crossing implies not only finding and fitting tracks from a collection of Pt "stubs" (hit pairs) but also continuosly reading out the stubs at 40MHz. The system needs to process 40 million beam crossings per second with a maximum latency of order of few microseconds. The total bandwidth required to transfer the stub data is of the order of 50-100 Tb/s and the total raw computational power needed to perform the track finding is huge, several orders of magnitude larger than what has ever been used for L1 triggering. We clearly need to utilize massive parallelism and we choose to do so by processing in parallel different crossings coming at different times (time multiplexing) and different regions of the detector for the same crossing (regional multiplexing). In such a parallel system, a significant problem to be solved is how to dispatch the right data to the right processors. Data from the same crossing, coming from different detector elements, must be assembled and delivered to the same processing unit for track reconstruction. Data from different crossings, coming from the same detector element, must be delivered to different processing units for optimal time multiplexing. The subdivision of the detector into geographical towers, does not lead to an exact corresponding subdivision of the track parameter space. Data coming from a given geographical trigger tower may need to be delivered to multiple parameter space regions. This happens, in particular, when a stub comes from a detector element close to the border between geographical trigger towers, due to the finite curvature of charged particles in the magnetic field and finite size of the beam luminous region along the beam axis. 

	In addition to the complex data dispatching, there is the obvious challenge of finding and fitting billions of tracks every second, which requires extremely fast pattern recognition algorithms. The Associative Memory (AM) approach uses a massively parallel architecture to tackle the intrinsically complex combinatorics of track finding algorithms, avoiding the typical power law dependance of execution time on occupancy and solving the pattern recognition in times roughly proportional to the number of hits. This is of crucial importance to be able to deal with the large occupancy fluctuations typical of hadronic collisions. However, the design of an Associative Memory system capable of dealing with the much higher complexity of the HL-LHC collisions, and with the much shorter latency required by Level 1 triggering, still poses significant technical challenges. For this reason, an aggressive R\&D program is ongoing at CMS to advance the state-of-the-art associative memory technology. Additionally, we are also open to new alternative approaches such as the tracklet-based approach. In this approach the pattern recognition is done by forming tracklets from pairs of stubs in any neighboring layers and matching stubs on a road defined by the direction of the tracklet and IP constraint. After the pattern recognition stage, in a way similar to the AM approach, a linearized track fit can be done using FPGAs. Feasibility of implementing the tracklet-based approach in modern FPGA is being studied. Because the Associative Memory approach is so far the only proven solution to tracking triggers in a hadron collider environment, it is chosen as the baseline. However, the architecture we are proposing is flexible to allow testing and comparing other possible alternatives, such as the tracklet-based approach.

	For the reasons above, the design of the overall architecture for CMS L1 tracking trigger has been focused on the need for efficient dispatching of the data for time and regional multiplexing and on the ability of providing a common flexible framework to test different possible solutions for track finding and fitting. Since the efficient data dispatching for time and regional multiplexing requires high bandwidth, low latency, and flexible real time communication among processing nodes, a full mesh backplane based hardware platform is a natural fit. A custom full mesh enabled ATCA board called Pulsar II has been designed at Fermilab with the goal of creating a scalable architecture abundant in flexible, non-blocking, high bandwidth board-to-board communication channels. Most of the hardware design work for the Puslar II has been done, including prototyping work. In addition, a pattern recognition mezzanine card will be designed for the demonstration, as the pattern recognition engine which can host a powerful FPGA and the new associative memory chips being developed at INFN (for Atlas FTK) and at Fermilab (for CMS L1), or simply a few FPGAs in the case of alternative pure FPGA-based track finding algorithms.  

	The full-mesh ATCA architecture permits high bandwidth inter-board communication. The full-mesh backplane is used to time-multiplex the high volume of incoming data in a way such that I/O demands are manageable at the board and chip level. For example, in the case of the AM approach, the default plan is to divide the detector into 48 angular regions (6 in $\eta$ times 8 in $\phi$) we call "trigger towers". We assign multiple processing engines to each tower so that data from that tower from different crossings may be processed in parallel. The architecture is scalable, flexible and will enable us to provide an early technical demonstration using existing technology. This ATCA architecture will allow us to explore and compare various pattern recognition architectures and algorithms within the same hardware platform. Given that Advanced Mezzanine Card (AMC) specifications are designed to work with both ATCA and microTCA, the architecture naturally allows for the long-term integration of Tracker DAQ (AMC based) and tracking trigger activities. 

	Our plan is to use the technology available today to build a vertical slice demonstration test bench to demonstrate track finding, identify possible bottlenecks and find solutions. This "Vertical Slice" will process simulated data with HL-LHC occupancy and speed and will allow us to study and improve the system performance (such as latency, efficiency and fake rate). Based on past experience, the final technology choices for the final implementation of L1 track finding can be delayed until about four years from the start of commissioning. The goal of this R\&D is to demonstrate that the track finding can be done so that the tracker detector design can be finalized in the near future. Once we can demonstrate a functional vertical slice with today's technology, Moore's law and the semiconductor industry will only work in our favor toward the HL-LHC era. This plan will help CMS to focus the attention on the real issues, compare different possible solutions to the fundamental pattern recognition and track fitting problems at HL-LHC era, and gain the necessary experience to move, in due time, toward the design of the final tracking trigger system.


=====  so far it is 1.5 pages. with some room for figures ===

perhaps to be added:

* Figure:   48 trigger tower with the interconnections among them. 
* Figure:   perhaps describe one trigger tower/crate and what's inside? 

perhaps explain Pt threshold of 2GeV here? or it will be done somewhere else?
explain the latency constraint here?  or will be done elsewhere? ... 
need few sentences on possible uTCA based architecture ....

